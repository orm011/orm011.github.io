<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Combining images with text for better CLIP-based image search | Oscar Moll </title> <meta name="author" content="Oscar Ricardo Moll"> <meta name="description" content="Combining images with text for better CLIP-based image search"> <meta name="keywords" content="data systems for ai, mit csail data systems group, data centric ai, academic website, personal website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%98&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://oscar-moll.com/blog/2024/combined-images-and-text/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Oscar Moll </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Combining images with text for better CLIP-based image search</h1> <p class="post-meta"> Created in July 17, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/machine-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> machine-learning</a>   <a href="/blog/tag/similarity-search"> <i class="fa-solid fa-hashtag fa-sm"></i> similarity-search</a>   <a href="/blog/tag/retrieval"> <i class="fa-solid fa-hashtag fa-sm"></i> retrieval</a>   <a href="/blog/tag/ai"> <i class="fa-solid fa-hashtag fa-sm"></i> AI</a>   <a href="/blog/tag/clip"> <i class="fa-solid fa-hashtag fa-sm"></i> CLIP</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p><em>Nearest neighbor search from image examples is the default approach for image searches, but there are better approaches, such as ExemplarSVM. Additionally, text-based searches using models like <a href="https://openai.com/index/clip/" rel="external nofollow noopener" target="_blank">CLIP</a> can be much more accurate than example-based searches. How can we get the best of both worlds? I show a starter approach.</em></p> <p>Semantic search is a key building block for working with image datasets. You need it when selecting images with specific attributes for labeling, when seeking to understand corner cases qualitatively, when profiling model errors, when seeking to extend datasets based on specific needs, or when cleaning your data. In all these cases you often need to locate relevant examples in your data, either based on similarity to some input image, or based on a description. Products like <a href="https://nucleus.scale.com/docs/getting-started" rel="external nofollow noopener" target="_blank">Scale.ai Nucleus</a> and <a href="https://docs.lilacml.com/datasets/dataset_explore.html#keyword-search" rel="external nofollow noopener" target="_blank">Lilac.ml</a> for example, offer <a href="https://nucleus.scale.com/docs/basic-similarity-search" rel="external nofollow noopener" target="_blank">search</a> as a key feature.</p> <p>The most direct way to implement these searches is via nearest neighbor vector search: vectors mathematically nearest to the vector representing the image example are returned as search results. More sophisticated methods include Exemplar SVM and text-based searches. All these methods all eventually reduce to a nearest neighbor lookup and the differences lie on what vector is used.</p> <h3 id="exemplar-svm">Exemplar SVM</h3> <p>ExemplarSVM is one such method, defined in <a href="https://icml.cc/2012/papers/946.pdf" rel="external nofollow noopener" target="_blank">this paper</a> and highlighted by Andrej Karpathy in this <a href="https://x.com/karpathy/status/1647025230546886658" rel="external nofollow noopener" target="_blank">tweet</a>. It works as follows:</p> <ol> <li>Use the input image example, \(\mathbf{x}\), as a positive example in a training set.</li> <li>Create negaive examples by sampling some database elements at random, even if we may be mislabeling some.</li> <li>Use this training set to fit a model with linear weights \(\mathbf{w}\), such as an SVM (support vector machine) model.</li> <li>Use the learned weight \(\mathbf{w}\) as the key for nearest neighbor lookup instead of the original \(\mathbf{x}\).</li> </ol> <p>The vector \(\mathbf{w}\) produces (hopefully) better quality results than the original vector \(\mathbf{x}\) without any extra labels or human input; this is a neat hello-world example for old-school semi-supervised learning.</p> <h3 id="text-based-searches">Text-based searches</h3> <p>Instead of using an image, we can also use CLIP to start searches with a text description. CLIP maps both images and text to a vector space where semantically similar text and images have higher dot products, so one can use either to start a nearest neighbor lookup. In practice CLIP works very well for text-based search; the <a href="https://arxiv.org/pdf/2103.00020" rel="external nofollow noopener" target="_blank">CLIP paper</a> shows CLIP zero-shot searches (ie. using text-based vectors) consistently deliver higher accuracy for classification than one-shot classifiers generated from an image example.</p> <p>If you havent tried CLIP yourself, there are a few good web-demos of CLIP-powered image searches like <a href="https://huggingface.co/spaces/vivien/clip" rel="external nofollow noopener" target="_blank">this one</a>. The demo lets you use both text-based and image-based searches; you can use a search bar for text or click on images to find similar ones. As far as I can tell, it does not combine these modalities.</p> <h3 id="comparing-these-approaches">Comparing these approaches</h3> <p>I compare how well these approaches work on a test dataset, and show a simple modification that combines both example-bsed and text-based approaches. I explain the benchmark details below, but the salient points of the benchmark results are the following:</p> <ol> <li>Exemplar SVM was only marginally better than kNN (unexpected)</li> <li>Text based search much better than example based search (the big gap in accuracy was unexpected, but the overall observation is consistent with the CLIP paper)</li> <li>Combining both modalities was clearly better than text search alone (unexpected given point 1)</li> </ol> <h3 id="combining-these-approaches">Combining these approaches</h3> <p>One simple method to combine the text and image based approaches is to use a modified SVM. Linear SVM is a linear model parametrized by a weight vector \(\mathbf{w}\) and a scalar bias (which is not important here). It is trained by minimizing the hinge-loss function:</p> \[\sum_{i=0}^{n} \max(0, 1 - y_i\cdot(\mathbf{w} \cdot \mathbf {x_i} + b)) + \lambda \frac{1}{2}||\mathbf{w}||^2\] <p>The initial sum is over model prediction errors. The \(y_i\) are the synthetic labels, with value \(+1\) or \(-1\). The second term is a penalty on making \(\mathbf{w}\) too large. \(\lambda\) is a hyperparameter weight on this penalty.</p> <p>One simple method to incorporate text information into Linear SVM is treating the text query vector \(\mathbf{q}\) as if it was just another positive example, but this resulted in overall worse accuracy than simply using the text vector alone. Instead, we modify the loss function by adding a second penalty term for large deviations from the text query vector \(\mathbf{q}\)</p> \[\lambda_q \left( 1 - \frac{\mathbf{q} \cdot \mathbf {w}}{||\mathbf{q}||\cdot ||\mathbf{w}||}\right)\] <p>where \(\lambda_q\) is a new hyperparamter weighting this second penalty term.</p> <p>The exact functional form of the distance turns out to not be super-relevant, but the idea of tying \(w\) to the query vector is consistently important in other experiments I’ve run. I implemented this model with PyTorch to accomodate the custom loss function, and you can read it <a href="https://github.com/orm011/playground/blob/main/playground/linear_model.py" rel="external nofollow noopener" target="_blank">in this file</a>.</p> <p>In the following experiments, setting \(\lambda = 10\) and \(\lambda_q = 1000\) worked best, and alterting them below an order of magnitude did not make a big difference. I tested the four different methods described so far on a quick benchmark based on the <a href="https://objectnet.dev/" rel="external nofollow noopener" target="_blank">ObjectNet dataset</a>. ObjecNet includes 50K images assigned into 300 categories, I used each category as a test query. I picked 10 positive samples at random for each category, used them as starting vectors \(x\) for example-based methods and used the remaining images as a test database. For the exemplar SVM method and the combined method, I additionaly used a sample of 1000 images from the test set as the negative examples, following the steps above. The exact size of this sample set was not critical for the results.</p> <p>For each query example we compute average precision (AP) scores. The mean AP is the mean over all runs of the experiment (about 3K). Average precision is a good metric to evaluate rankings compared to pure precision or recall because it considers the full ordering of the results.</p> <p>The code, data, and benchmarks are available in <a href="https://github.com/orm011/playground/blob/main/svm_text_exp.ipynb" rel="external nofollow noopener" target="_blank">this notebook and repo</a>, and I copy the results below from the notebook.</p> <style>table{width:100%;border-collapse:collapse}</style> <table> <thead> <tr> <th style="text-align: center">Search Method</th> <th style="text-align: center">Mean Average Precision (mAP)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Image-based Nearest Neighbor</td> <td style="text-align: center">0.094</td> </tr> <tr> <td style="text-align: center">Exemplar SVM</td> <td style="text-align: center">0.099</td> </tr> <tr> <td style="text-align: center">Text-based Nearest Neighbor</td> <td style="text-align: center">0.237</td> </tr> <tr> <td style="text-align: center">Combined exemplar SVM + text</td> <td style="text-align: center"><strong>0.251</strong></td> </tr> </tbody> </table> <p><br> I was suprised that ExemplarSVM only marginally beat image-based nearest neighbor. Perhaps the bigger problem with exemplar SVM in this benchmark is that over the 300 categories, ExemplarSVM was better than kNN on less than 60% of them, which may make it too unpredictable to be worth implementing in practice. On the other hand, the combined approach works better than the text-based 80% of the time. For both approaches, more positive examples will probably create a more consistent improvement.</p> <p>It is possible the ObjectNet dataset makes text-based search appear stronger than it can be in the wild, because the ObjectNet dataset itself was collected from a pre-specified set of easily stated classes; its contents cluster around 300 concepts, and these concepts by design correspond to objects with well known names.</p> <h3 id="extensions">Extensions</h3> <p>I develop related ideas more deeply in SeeSaw <a class="citation" href="#seesaw">(Moll et al., 2023)</a>, a system that reduces the amount of feedback users need to provide in order to improve their image search results. SeeSaw tackles this problem by leveraging different kinds of image representation and semi-supervised learning techniques, some show up as loss function modifications like the one above.</p> <p>The experiments with text based and image based results also suggest text representations may be a better intermediate form than pure examples for some kinds of searches. Hence, now that GPT4V can easily generate captions for images, we may be able to use these as intermdiate search representations without requiring extra human input.</p> <p>Beyond single lookup searches and simple binary feedback, it would be great to be able to provide a variety of verbal feedback on results, explaining why something is or is not a good result, and improve results that way. Current Visual Language models like GPT4V let you ask questions about input images, but retrieval over your own database of images is not yet an option.</p> </div> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#009f36"> <a href="https://2024.sigmod.org/" rel="external nofollow noopener" target="_blank">ACM SIGMOD</a> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/SeeSaw_preview-480.webp 480w,/assets/img/publication_preview/SeeSaw_preview-800.webp 800w,/assets/img/publication_preview/SeeSaw_preview-1400.webp 1400w," sizes="200px" type="image/webp"> <img src="/assets/img/publication_preview/SeeSaw_preview.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="SeeSaw_preview.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="seesaw" class="col-sm-8"> <div class="title">SeeSaw: Interactive Ad-hoc Search Over Image Databases</div> <div class="author"> <em>Oscar Moll</em>, Manuel Favela, Samuel Madden, Vijay Gadepally, and Michael Cafarella </div> <div class="periodical"> <em>Proc. ACM Manag. Data</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/Moll_SeeSaw_SIGMOD.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p> As image datasets become ubiquitous, the problem of ad-hoc searches over image data is increasingly important. Many high-level data tasks in machine learning, such as constructing datasets for training and testing object detectors, imply finding ad-hoc objects or scenes within large image datasets as a key sub-problem. New foundational visual-semantic embeddings trained on massive web datasets such as Contrastive Language-Image Pre-Training (CLIP) can help users start searches on their own data, but we find there is a long tail of queries where these models fall short in practice. Seesaw is a system for interactive ad-hoc searches on image datasets that integrates state-of-the-art embeddings like CLIP with user feedback in the form of box annotations to help users quickly locate images of interest in their data even in the long tail of harder queries. One key challenge for Seesaw is that, in practice, many sensible approaches to incorporating feedback into future results, including state-of-the-art active-learning algorithms, can worsen results compared to introducing no feedback, partly due to CLIP’s high-average performance. Therefore, Seesaw includes several algorithms that empirically result in larger and also more consistent improvements. We compare Seesaw’s accuracy to both using CLIP alone and to a state-of-the-art active-learning baseline and find Seesaw consistently helps improve results for users across four datasets and more than a thousand queries. Seesaw increases Average Precision (AP) on search tasks by an average of .08 on a wide benchmark (from a base of .72), and by a .27 on a subset of more difficult queries where CLIP alone performs poorly. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">seesaw</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Moll, Oscar and Favela, Manuel and Madden, Samuel and Gadepally, Vijay and Cafarella, Michael}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SeeSaw: Interactive Ad-hoc Search Over Image Databases}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">issue_date</span> <span class="p">=</span> <span class="s">{December 2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{New York, NY, USA}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1145/3626754}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1145/3626754}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Proc. ACM Manag. Data}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">dec</span><span class="p">,</span>
  <span class="na">articleno</span> <span class="p">=</span> <span class="s">{260}</span><span class="p">,</span>
  <span class="na">numpages</span> <span class="p">=</span> <span class="s">{26}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{active search, data exploration, image retrieval, image search, machine learning, multi-modal embeddings, relevance feedback}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"orm011/orm011.github.io","data-repo-id":"R_kgDOMDVr2Q","data-category":"Announcements","data-category-id":"DIC_kwDOMDVr2c4CgdZw","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Oscar Ricardo Moll. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: July 24, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>
<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://oscar-moll.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://oscar-moll.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-24T17:30:02+00:00</updated><id>https://oscar-moll.com/feed.xml</id><title type="html">Oscar Moll</title><subtitle>Oscar Moll&apos;s academic/personal website. I am a PhD student at MIT CSAIL working on data systems for AI. </subtitle><entry><title type="html">Combining images with text for better CLIP-based image search</title><link href="https://oscar-moll.com/blog/2024/combined-images-and-text/" rel="alternate" type="text/html" title="Combining images with text for better CLIP-based image search"/><published>2024-07-17T12:57:00+00:00</published><updated>2024-07-17T12:57:00+00:00</updated><id>https://oscar-moll.com/blog/2024/combined-images-and-text</id><content type="html" xml:base="https://oscar-moll.com/blog/2024/combined-images-and-text/"><![CDATA[<p><em>Nearest neighbor search from image examples is the default approach for image searches, but there are better approaches, such as ExemplarSVM. Additionally, text-based searches using models like <a href="https://openai.com/index/clip/">CLIP</a> can be much more accurate than example-based searches. How can we get the best of both worlds? I show a starter approach.</em></p> <p>Semantic search is a key building block for working with image datasets. You need it when selecting images with specific attributes for labeling, when seeking to understand corner cases qualitatively, when profiling model errors, when seeking to extend datasets based on specific needs, or when cleaning your data. In all these cases you often need to locate relevant examples in your data, either based on similarity to some input image, or based on a description. Products like <a href="https://nucleus.scale.com/docs/getting-started">Scale.ai Nucleus</a> and <a href="https://docs.lilacml.com/datasets/dataset_explore.html#keyword-search">Lilac.ml</a> for example, offer <a href="https://nucleus.scale.com/docs/basic-similarity-search">search</a> as a key feature.</p> <p>The most direct way to implement these searches is via nearest neighbor vector search: vectors mathematically nearest to the vector representing the image example are returned as search results. More sophisticated methods include Exemplar SVM and text-based searches. All these methods all eventually reduce to a nearest neighbor lookup and the differences lie on what vector is used.</p> <h3 id="exemplar-svm">Exemplar SVM</h3> <p>ExemplarSVM is one such method, defined in <a href="https://icml.cc/2012/papers/946.pdf">this paper</a> and highlighted by Andrej Karpathy in this <a href="https://x.com/karpathy/status/1647025230546886658">tweet</a>. It works as follows:</p> <ol> <li>Use the input image example, \(\mathbf{x}\), as a positive example in a training set.</li> <li>Create negaive examples by sampling some database elements at random, even if we may be mislabeling some.</li> <li>Use this training set to fit a model with linear weights \(\mathbf{w}\), such as an SVM (support vector machine) model.</li> <li>Use the learned weight \(\mathbf{w}\) as the key for nearest neighbor lookup instead of the original \(\mathbf{x}\).</li> </ol> <p>The vector \(\mathbf{w}\) produces (hopefully) better quality results than the original vector \(\mathbf{x}\) without any extra labels or human input; this is a neat hello-world example for old-school semi-supervised learning.</p> <h3 id="text-based-searches">Text-based searches</h3> <p>Instead of using an image, we can also use CLIP to start searches with a text description. CLIP maps both images and text to a vector space where semantically similar text and images have higher dot products, so one can use either to start a nearest neighbor lookup. In practice CLIP works very well for text-based search; the <a href="https://arxiv.org/pdf/2103.00020">CLIP paper</a> shows CLIP zero-shot searches (ie. using text-based vectors) consistently deliver higher accuracy for classification than one-shot classifiers generated from an image example.</p> <p>If you havent tried CLIP yourself, there are a few good web-demos of CLIP-powered image searches like <a href="https://huggingface.co/spaces/vivien/clip">this one</a>. The demo lets you use both text-based and image-based searches; you can use a search bar for text or click on images to find similar ones. As far as I can tell, it does not combine these modalities.</p> <h3 id="comparing-these-approaches">Comparing these approaches</h3> <p>I compare how well these approaches work on a test dataset, and show a simple modification that combines both example-bsed and text-based approaches. I explain the benchmark details below, but the salient points of the benchmark results are the following:</p> <ol> <li>Exemplar SVM was only marginally better than kNN (unexpected)</li> <li>Text based search much better than example based search (the big gap in accuracy was unexpected, but the overall observation is consistent with the CLIP paper)</li> <li>Combining both modalities was clearly better than text search alone (unexpected given point 1)</li> </ol> <h3 id="combining-these-approaches">Combining these approaches</h3> <p>One simple method to combine the text and image based approaches is to use a modified SVM. Linear SVM is a linear model parametrized by a weight vector \(\mathbf{w}\) and a scalar bias (which is not important here). It is trained by minimizing the hinge-loss function:</p> \[\sum_{i=0}^{n} \max(0, 1 - y_i\cdot(\mathbf{w} \cdot \mathbf {x_i} + b)) + \lambda \frac{1}{2}||\mathbf{w}||^2\] <p>The initial sum is over model prediction errors. The \(y_i\) are the synthetic labels, with value \(+1\) or \(-1\). The second term is a penalty on making \(\mathbf{w}\) too large. \(\lambda\) is a hyperparameter weight on this penalty.</p> <p>One simple method to incorporate text information into Linear SVM is treating the text query vector \(\mathbf{q}\) as if it was just another positive example, but this resulted in overall worse accuracy than simply using the text vector alone. Instead, we modify the loss function by adding a second penalty term for large deviations from the text query vector \(\mathbf{q}\)</p> \[\lambda_q \left( 1 - \frac{\mathbf{q} \cdot \mathbf {w}}{||\mathbf{q}||\cdot ||\mathbf{w}||}\right)\] <p>where \(\lambda_q\) is a new hyperparamter weighting this second penalty term.</p> <p>The exact functional form of the distance turns out to not be super-relevant, but the idea of tying \(w\) to the query vector is consistently important in other experiments Iâ€™ve run. I implemented this model with PyTorch to accomodate the custom loss function, and you can read it <a href="https://github.com/orm011/playground/blob/main/playground/linear_model.py">in this file</a>.</p> <p>In the following experiments, setting \(\lambda = 10\) and \(\lambda_q = 1000\) worked best, and alterting them below an order of magnitude did not make a big difference. I tested the four different methods described so far on a quick benchmark based on the <a href="https://objectnet.dev/">ObjectNet dataset</a>. ObjecNet includes 50K images assigned into 300 categories, I used each category as a test query. I picked 10 positive samples at random for each category, used them as starting vectors \(x\) for example-based methods and used the remaining images as a test database. For the exemplar SVM method and the combined method, I additionaly used a sample of 1000 images from the test set as the negative examples, following the steps above. The exact size of this sample set was not critical for the results.</p> <p>For each query example we compute average precision (AP) scores. The mean AP is the mean over all runs of the experiment (about 3K). Average precision is a good metric to evaluate rankings compared to pure precision or recall because it considers the full ordering of the results.</p> <p>The code, data, and benchmarks are available in <a href="https://github.com/orm011/playground/blob/main/svm_text_exp.ipynb">this notebook and repo</a>, and I copy the results below from the notebook.</p> <style>table{width:100%;border-collapse:collapse}</style> <table> <thead> <tr> <th style="text-align: center">Search Method</th> <th style="text-align: center">Mean Average Precision (mAP)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">Image-based Nearest Neighbor</td> <td style="text-align: center">0.094</td> </tr> <tr> <td style="text-align: center">Exemplar SVM</td> <td style="text-align: center">0.099</td> </tr> <tr> <td style="text-align: center">Text-based Nearest Neighbor</td> <td style="text-align: center">0.237</td> </tr> <tr> <td style="text-align: center">Combined exemplar SVM + text</td> <td style="text-align: center"><strong>0.251</strong></td> </tr> </tbody> </table> <p><br/> I was suprised that ExemplarSVM only marginally beat image-based nearest neighbor. Perhaps the bigger problem with exemplar SVM in this benchmark is that over the 300 categories, ExemplarSVM was better than kNN on less than 60% of them, which may make it too unpredictable to be worth implementing in practice. On the other hand, the combined approach works better than the text-based 80% of the time. For both approaches, more positive examples will probably create a more consistent improvement.</p> <p>It is possible the ObjectNet dataset makes text-based search appear stronger than it can be in the wild, because the ObjectNet dataset itself was collected from a pre-specified set of easily stated classes; its contents cluster around 300 concepts, and these concepts by design correspond to objects with well known names.</p> <h3 id="extensions">Extensions</h3> <p>I develop related ideas more deeply in SeeSaw <a class="citation" href="#seesaw">(Moll et al., 2023)</a>, a system that reduces the amount of feedback users need to provide in order to improve their image search results. SeeSaw tackles this problem by leveraging different kinds of image representation and semi-supervised learning techniques, some show up as loss function modifications like the one above.</p> <p>The experiments with text based and image based results also suggest text representations may be a better intermediate form than pure examples for some kinds of searches. Hence, now that GPT4V can easily generate captions for images, we may be able to use these as intermdiate search representations without requiring extra human input.</p> <p>Beyond single lookup searches and simple binary feedback, it would be great to be able to provide a variety of verbal feedback on results, explaining why something is or is not a good result, and improve results that way. Current Visual Language models like GPT4V let you ask questions about input images, but retrieval over your own database of images is not yet an option.</p>]]></content><author><name></name></author><category term="machine-learning"/><category term="similarity-search"/><category term="retrieval"/><category term="AI"/><category term="CLIP"/><summary type="html"><![CDATA[Combining images with text for better CLIP-based image search]]></summary></entry></feed>